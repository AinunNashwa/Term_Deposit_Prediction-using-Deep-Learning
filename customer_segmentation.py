# -*- coding: utf-8 -*-
"""customer_segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nMj0uA2c9OjDOubMNTicoulWVCxtV-aG
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Jun 22 09:29:29 2022
This script is to develop a Deep Learning model to predict 
the outcome of the campaign

1) id:Unique identifier for each sample in the dataset. Cannot be used for modelling

2) customer_age:Age of the Customer in years

3) job_type: Type of job of the customer

4) marital: Marital Status of the Customer

5) education: Education Level of the Customer

6) default: Whether customer has Defaulted in Past

7) balance: Current Balance in the Customer's Bank

8) housing_loan: Has customer taken a Housing Loan

9) personal_loan: Has customer taken a Personal Loan

10) communication_type: Type of communication made by the bank with the customer

11) day_of_month: Day of month of the last contact made with customer

12) month: Month for the last contact made with customer

13) last_contact_duration: Last Contact duration made with the customer (in seconds)

14) num_contacts_in_campaign: Number of contacts made with the customer during the current campaign.

15) days_since_prev_campaign_contact:Number of days passed since customer was contacted in previous campaign.
    
16) num_contacts_prev_campaign:Number of contacts made with the customer during the previous campaign.
    
17) prev_campaign_outcome:Success or Failure in previous Campaign.
    
18) term_deposit_subscribed:(Target) Has the customer taken a term deposit ?

@author: User
"""

!cp /content/drive/MyDrive/Colab\ Notebooks/modules_cust_segmentation.py/content

from modules_cust_segmentation import CramersV,ModelCreation,model_evaluation
from sklearn.preprocessing import LabelEncoder,StandardScaler,OneHotEncoder
from tensorflow.keras.layers import Input,Dense,Dropout,BatchNormalization
from sklearn.metrics import confusion_matrix,classification_report
from tensorflow.keras.callbacks import TensorBoard,EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import ConfusionMatrixDisplay
from tensorflow.keras.utils import plot_model
from tensorflow.keras import Sequential

import matplotlib.pyplot as plt
import scipy.stats as ss
import seaborn as sns
import pandas as pd
import numpy as np
import datetime
import pickle
import os

#%% Statics
CSV_PATH = os.path.join(os.getcwd(),'/content/sample_data/Cust_Segmentation/dataset/Train.csv')
LE_PICKLE_PATH = os.path.join(os.getcwd(),'le_cust_segment.pkl')
SS_PICKLE_PATH = os.path.join(os.getcwd(),'ss_cust_segmentation.pkl')
OHE_PICKLE_PATH = os.path.join(os.getcwd(),'ohe_cust_segmentation.pkl')

log_dir = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
LOG_FOLDER_PATH = os.path.join(os.getcwd(), '/content/sample_data/Cust_Segmentation/logs_segmentation',log_dir)

#%% Functions

# calling f(x) from modules
cv = CramersV()
cv.cramers_corrected_stat

#%% EDA

# Step 1: Data Loading

# contains empty columns, unknown and other labels
df = pd.read_csv(CSV_PATH, na_values='null')

# Step 2: Data Inspection
df.info()
df['term_deposit_subscribed'] = df['term_deposit_subscribed'].astype(str) 

df.info()
df_describe = df.describe().T

df.boxplot(figsize=(25,10))
# from the boxplot, there are outliers at balance,last_contact_duration

# to check nan & duplicated data
df.isna().sum() # customer_age,marital,balance,personal_loan,last_contact_duration,
                # num_contacts_in_campaign,days_since_prev_campaign_contact
                
df.duplicated().sum() # 0

## Drop ID columns/Drop columns with many NaNs

df = df.drop(labels=['id','days_since_prev_campaign_contact'],axis=1)
df.info()

categorical_columns = df.columns[df.dtypes=='object']
continuos_columns = df.columns[(df.dtypes=='int64') | (df.dtypes=='float64')]

##categorical data
for cat in categorical_columns:
    plt.figure(figsize=(12,10))
    sns.countplot(df[cat])
    plt.show()

##continuos data
for cont in continuos_columns:
    plt.figure(figsize=(12,10))
    sns.distplot(df[cont])
    plt.show()

# applicable when y is categorical
for cat in categorical_columns:
    plt.figure(figsize=(12,10))
    sns.countplot(df[cat],hue=df['term_deposit_subscribed'])
    plt.show()

# Step 3: Data Cleaning
## convert categorical data to_numeric

le = LabelEncoder()

for index,cat in enumerate (categorical_columns):
    temp = df[cat]
    temp[temp.notnull()]=le.fit_transform(temp[temp.notnull()])
    df[cat] = pd.to_numeric(temp,errors='coerce')

df.info()

# to make sure there is no Decimal Places in categorical data
df['customer_age'] = np.floor(df['customer_age'])

df.info()

### save the model
with open(LE_PICKLE_PATH, 'wb') as file:
    pickle.dump(le,file)

## Impute the NaN values: Simple Imputer

for cat in categorical_columns:
    df[cat] = df[cat].fillna(df[cat].mode()[0])
                             
for cont in continuos_columns:
    df[cont] = df[cont].fillna(df[cont].median())

df.isna().sum()

# Step 4: Features Selection

# Continuos vs Categorical : Logistic Regression

for cont in continuos_columns:
    lr = LogisticRegression(solver='liblinear')
    lr.fit(np.expand_dims(df[cont],axis=-1),df['term_deposit_subscribed'])

    print(cont)
    print(lr.score(np.expand_dims(df[cont],axis=-1),df['term_deposit_subscribed']))

# customer_age: 0.892754447498973
# balance: 0.892754447498973
# day_of_month: 0.892754447498973
# last_contact_duration: 0.9001485132871995
# num_contacts_in_campaign: 0.892754447498973
# num_contacts_prev_campaign: 0.8912693146269789

# Categorical Vs Categorical : Cramer's V

for cat in categorical_columns:
    print(cat)
    confussion_mat = pd.crosstab(df[cat],df['term_deposit_subscribed']).to_numpy()
    print(cramers_corrected_stat(confussion_mat))

# all categorical data have low correlation with the target
# the highest correclation is prev_campaign_outcome with0.3410607961880476

# Step 5: Preprocessing
X = df.loc[:,['customer_age','balance','day_of_month','last_contact_duration','num_contacts_in_campaign','num_contacts_prev_campaign']]
y = df['term_deposit_subscribed']

#Features Scalling

ss = StandardScaler()
X = ss.fit_transform(X)

nb_features = np.shape(X)[1:]
nb_classes = len(np.unique(y))

# save ss model
with open(SS_PICKLE_PATH, 'wb') as file:
    pickle.dump(ss,file)

# One-Hot-Encoding
ohe = OneHotEncoder(sparse=False)
y = ohe.fit_transform(np.expand_dims(y,axis=1))

X_train,X_test,y_train,y_test = train_test_split(X,y,
                                                 test_size=0.3,
                                                 random_state=123)
# save the ohe model
with open(OHE_PICKLE_PATH, 'wb') as file:
    pickle.dump(ohe,file)

#%% Model Development
# Sequential

### calling f(x) from modules
mc = ModelCreation()
model = mc.simple_lstm_layer(nb_features,nb_classes,num_node=128,drop_rate=0.3,output_node=1)

# wrap the container
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics='acc')

# callback

tensorboard_callback = TensorBoard(log_dir=LOG_FOLDER_PATH)
early_stopping_callback = EarlyStopping(monitor='loss',patience=3)

# plot
hist = model.fit(X_train,y_train,
                 batch_size=128,
                 epochs=100,
                 validation_data=(X_test,y_test),
                 callbacks=[tensorboard_callback,early_stopping_callback])

hist.history.keys()

## calling f(x) from modules
# plotting the graph
me = model_evaluation()
me.plot_graph(hist)

# Model Evaluation
print(model.evaluate(X_test,y_test))

y_true = np.argmax(y_test,axis=1)
y_pred = np.argmax(model.predict(X_test),axis=1)

cr = classification_report(y_true,y_pred)
cm = confusion_matrix(y_true,y_pred)

print(cr)
print(cm)

plot_model(model,show_shapes=True,show_layer_names=(True))

labels = ['Not Taken Deposit','Taken Deposit']
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labels)
disp.plot(cmap=plt.cm.Blues)
plt.show()

#%% model saving

MODEL_SAVE_PATH = os.path.join(os.getcwd(),'model_cust_segmentation.h5')

model.save(MODEL_SAVE_PATH)

#%% Discussion
# The dataset contains more than 30,000 datasets with 18 attributes
# The dataset also contains null and undefined category(assume undefined category 
# as the number of category in that specific attributes)
# For specific columns contains to many NaNs up to 20,000 and 
# for this case we drop that column
# Then data need to be transform in numeric and scaled.
# The target is in categorical data and the dataset contains 
# both categorical and continuous data
# For the features selection, all continuous data being selected 
# as their accuracy approximately 90%
# But for the categorical data, the correlation is quite low from the target, 
# less than 0.5, so in this case none of category data being selected
# The model produced 90% accuracy which is quite good.

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard
# %reload_ext tensorboard
# %tensorboard --logdir /content/sample_data/Cust_Segmentation/logs_segmentation